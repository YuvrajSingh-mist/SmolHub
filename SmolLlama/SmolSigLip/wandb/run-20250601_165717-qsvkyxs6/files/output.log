‚úÖ Wandb initialized successfully!
üìä Project: SmolSigLip
üèÉ Run: siglip_lr0.0004_bs16_epochs70000
üÜî Run ID: qsvkyxs6
üåê Dashboard: https://wandb.ai/rentio/SmolSigLip/runs/qsvkyxs6
üîß Setting up custom metrics...
‚úÖ All metrics defined successfully!
Model logged to wandb - Total params: 212,986,602, Trainable: 212,986,602
üéØ Epoch 1/70000:   0%|[35m                                                                          [0m| 0/70000 [00:00<?, ?epoch/s][0mW0601 16:57:38.304000 924380 site-packages/torch/_dynamo/convert_frame.py:964] [22/8] torch._dynamo hit config.recompile_limit (8)

=== EPOCH 1/70000 ===
W0601 16:57:38.304000 924380 site-packages/torch/_dynamo/convert_frame.py:964] [22/8]    function: 'torch_dynamo_resume_in_valid_images_at_150' (/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/image_utils.py:150)
W0601 16:57:38.304000 924380 site-packages/torch/_dynamo/convert_frame.py:964] [22/8]    last reason: 22/7: ___tuple_iterator_len(___stack0) == 8
W0601 16:57:38.304000 924380 site-packages/torch/_dynamo/convert_frame.py:964] [22/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W0601 16:57:38.304000 924380 site-packages/torch/_dynamo/convert_frame.py:964] [22/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
üéØ Epoch 1/70000:   0%|[35m                                                                          [0m| 0/70000 [00:19<?, ?epoch/s][0m
Traceback (most recent call last):                                                                                            
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/train.py", line 467, in <module>
    results = engine.train(model=siglip,
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/engine.py", line 333, in train
    train_loss, train_acc = train_step(model=model,
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/engine.py", line 72, in train_step
    y_pred = model(batch)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/train.py", line 351, in forward
    embeds_text = self.text(batch['text'])
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/train.py", line 354, in torch_dynamo_resume_in_forward_at_351
    embeds_img = self.vision(batch['image'])
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/train.py", line 330, in forward
    inputs = self.preprocessor(images=x, return_tensors="pt")
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/image_processing_utils.py", line 44, in __call__
    return self.preprocess(images, **kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/utils/generic.py", line 870, in wrapper
    return func(*args, **valid_kwargs)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 228, in preprocess
    images = make_list_of_images(images)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 230, in torch_dynamo_resume_in_preprocess_at_228
    if not valid_images(images):
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 250, in torch_dynamo_resume_in_preprocess_at_230
    images = [to_numpy_array(image) for image in images]
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 252, in torch_dynamo_resume_in_preprocess_at_250
    if do_rescale and is_scaled_image(images[0]):
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 252, in torch_dynamo_resume_in_preprocess_at_252
    if do_rescale and is_scaled_image(images[0]):
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 263, in torch_dynamo_resume_in_preprocess_at_252
    images = [
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 264, in <listcomp>
    self.resize(image=image, size=size_dict, resample=resample, input_data_format=input_data_format)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py", line 144, in resize
    return resize(
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/image_transforms.py", line 355, in resize
    requires_backends(resize, ["vision"])
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/image_transforms.py", line 372, in torch_dynamo_resume_in_resize_at_355
    do_rescale = _rescale_for_pil_conversion(image)
  File "/speech/advait/miniconda3/envs/trainBigLlama/lib/python3.10/site-packages/transformers/image_transforms.py", line 154, in _rescale_for_pil_conversion
    raise ValueError(
ValueError: The image to be converted to a PIL image contains values outside the range [0, 1], got [-1.8952821493148804, 2.6399998664855957] which cannot be converted to uint8.
