✅ Wandb initialized successfully!
📊 Project: SmolSigLip
🏃 Run: siglip_lr0.0004_bs32_epochs30
🆔 Run ID: 7qnsy4yo
🌐 Dashboard: https://wandb.ai/rentio/SmolSigLip/runs/7qnsy4yo
🔧 Setting up custom metrics...
✅ All metrics defined successfully!
Model logged to wandb - Total params: 150,910,018, Trainable: 150,910,018
🎯 Epoch 1/30:   0%|[35m                                                                                [0m| 0/30 [00:00<?, ?epoch/s][0m

=== EPOCH 1/30 ===
🚀 Epoch 01 Training:  10%|[32m██▍                     [0m| 31/313 [00:09<01:05,  4.31it/s, Loss=4.5217, LR=1.00e-04, BS=32, Step=31][0m
🚀 Epoch 01/30 | Step 00000 | Batch 001/313 | Loss: 10.434273 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00001 | Batch 002/313 | Loss: 4.913714 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00002 | Batch 003/313 | Loss: 6.522486 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00003 | Batch 004/313 | Loss: 6.058648 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00004 | Batch 005/313 | Loss: 4.746779 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00005 | Batch 006/313 | Loss: 4.587910 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00006 | Batch 007/313 | Loss: 5.170021 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00007 | Batch 008/313 | Loss: 4.908714 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00008 | Batch 009/313 | Loss: 4.520286 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00009 | Batch 010/313 | Loss: 4.494875 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00010 | Batch 011/313 | Loss: 4.678600 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00011 | Batch 012/313 | Loss: 4.794274 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00012 | Batch 013/313 | Loss: 4.774096 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00013 | Batch 014/313 | Loss: 4.593920 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00014 | Batch 015/313 | Loss: 4.457264 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00015 | Batch 016/313 | Loss: 4.497955 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00016 | Batch 017/313 | Loss: 4.604567 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00017 | Batch 018/313 | Loss: 4.614951 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00018 | Batch 019/313 | Loss: 4.504737 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00019 | Batch 020/313 | Loss: 4.457041 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00020 | Batch 021/313 | Loss: 4.504695 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00021 | Batch 022/313 | Loss: 4.652510 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00022 | Batch 023/313 | Loss: 4.616344 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00023 | Batch 024/313 | Loss: 4.480033 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00024 | Batch 025/313 | Loss: 4.474356 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00025 | Batch 026/313 | Loss: 4.581148 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00026 | Batch 027/313 | Loss: 4.567099 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00027 | Batch 028/313 | Loss: 4.460861 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00028 | Batch 029/313 | Loss: 4.533809 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00029 | Batch 030/313 | Loss: 4.567518 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00030 | Batch 031/313 | Loss: 4.477981 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00031 | Batch 032/313 | Loss: 4.521652 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00032 | Batch 033/313 | Loss: 4.563803 | LR: 0.00010000 | BS: 32

🚀 Epoch 01 Training:  20%|[32m████▉                   [0m| 64/313 [00:16<00:57,  4.33it/s, Loss=4.6831, LR=1.00e-04, BS=32, Step=64][0m
🚀 Epoch 01/30 | Step 00033 | Batch 034/313 | Loss: 4.459795 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00034 | Batch 035/313 | Loss: 4.545734 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00035 | Batch 036/313 | Loss: 4.500653 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00036 | Batch 037/313 | Loss: 4.498925 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00037 | Batch 038/313 | Loss: 4.520513 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00038 | Batch 039/313 | Loss: 4.496744 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00039 | Batch 040/313 | Loss: 4.527940 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00040 | Batch 041/313 | Loss: 4.494056 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00041 | Batch 042/313 | Loss: 4.460913 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00042 | Batch 043/313 | Loss: 4.478467 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00043 | Batch 044/313 | Loss: 4.456022 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00044 | Batch 045/313 | Loss: 4.477485 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00045 | Batch 046/313 | Loss: 4.476289 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00046 | Batch 047/313 | Loss: 4.455782 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00047 | Batch 048/313 | Loss: 4.519537 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00048 | Batch 049/313 | Loss: 4.627421 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00049 | Batch 050/313 | Loss: 4.580816 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00050 | Batch 051/313 | Loss: 4.953545 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00051 | Batch 052/313 | Loss: 5.503921 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00052 | Batch 053/313 | Loss: 5.955210 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00053 | Batch 054/313 | Loss: 5.800152 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00054 | Batch 055/313 | Loss: 4.484366 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00055 | Batch 056/313 | Loss: 5.640204 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00056 | Batch 057/313 | Loss: 4.503259 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00057 | Batch 058/313 | Loss: 5.179276 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00058 | Batch 059/313 | Loss: 4.967611 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00059 | Batch 060/313 | Loss: 4.461407 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00060 | Batch 061/313 | Loss: 4.746361 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00061 | Batch 062/313 | Loss: 4.742852 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00062 | Batch 063/313 | Loss: 4.468596 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00063 | Batch 064/313 | Loss: 4.588018 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00064 | Batch 065/313 | Loss: 4.683115 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00065 | Batch 066/313 | Loss: 4.639631 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00066 | Batch 067/313 | Loss: 4.462848 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00067 | Batch 068/313 | Loss: 4.571364 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00068 | Batch 069/313 | Loss: 4.621331 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00069 | Batch 070/313 | Loss: 4.517898 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00070 | Batch 071/313 | Loss: 4.468758 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00071 | Batch 072/313 | Loss: 4.550139 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00072 | Batch 073/313 | Loss: 4.581795 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00073 | Batch 074/313 | Loss: 4.571545 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00074 | Batch 075/313 | Loss: 4.463769 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00075 | Batch 076/313 | Loss: 4.537414 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00076 | Batch 077/313 | Loss: 4.554687 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00077 | Batch 078/313 | Loss: 4.479309 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00078 | Batch 079/313 | Loss: 4.476237 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00079 | Batch 080/313 | Loss: 4.529446 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00080 | Batch 081/313 | Loss: 4.542306 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00081 | Batch 082/313 | Loss: 4.474794 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00082 | Batch 083/313 | Loss: 4.496718 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00083 | Batch 084/313 | Loss: 4.556314 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00084 | Batch 085/313 | Loss: 4.487155 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00085 | Batch 086/313 | Loss: 4.480677 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00086 | Batch 087/313 | Loss: 4.557859 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00087 | Batch 088/313 | Loss: 4.480358 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00088 | Batch 089/313 | Loss: 4.460312 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00089 | Batch 090/313 | Loss: 4.501954 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00090 | Batch 091/313 | Loss: 4.478411 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00091 | Batch 092/313 | Loss: 4.459903 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00092 | Batch 093/313 | Loss: 4.481728 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00093 | Batch 094/313 | Loss: 4.464830 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00094 | Batch 095/313 | Loss: 4.453854 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00095 | Batch 096/313 | Loss: 4.487688 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00096 | Batch 097/313 | Loss: 4.474979 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00097 | Batch 098/313 | Loss: 4.482222 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00098 | Batch 099/313 | Loss: 4.501179 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00099 | Batch 100/313 | Loss: 4.458278 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00100 | Batch 101/313 | Loss: 4.503411 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00101 | Batch 102/313 | Loss: 4.455227 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00102 | Batch 103/313 | Loss: 4.495489 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00103 | Batch 104/313 | Loss: 4.456128 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00104 | Batch 105/313 | Loss: 4.518554 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00105 | Batch 106/313 | Loss: 4.453964 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00106 | Batch 107/313 | Loss: 4.505713 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00107 | Batch 108/313 | Loss: 4.452254 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00108 | Batch 109/313 | Loss: 4.532333 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00109 | Batch 110/313 | Loss: 4.491289 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00110 | Batch 111/313 | Loss: 4.484793 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00111 | Batch 112/313 | Loss: 4.502032 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00112 | Batch 113/313 | Loss: 4.454956 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00113 | Batch 114/313 | Loss: 4.478317 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00114 | Batch 115/313 | Loss: 4.456133 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00115 | Batch 116/313 | Loss: 4.467937 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00116 | Batch 117/313 | Loss: 4.476035 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00117 | Batch 118/313 | Loss: 4.455071 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00118 | Batch 119/313 | Loss: 4.467918 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00119 | Batch 120/313 | Loss: 4.464982 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00120 | Batch 121/313 | Loss: 4.453278 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00121 | Batch 122/313 | Loss: 4.455983 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00122 | Batch 123/313 | Loss: 4.457947 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00123 | Batch 124/313 | Loss: 4.454839 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00124 | Batch 125/313 | Loss: 4.454386 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00125 | Batch 126/313 | Loss: 4.453300 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00126 | Batch 127/313 | Loss: 4.456292 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00127 | Batch 128/313 | Loss: 4.455993 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00128 | Batch 129/313 | Loss: 4.459845 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00129 | Batch 130/313 | Loss: 4.470118 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00130 | Batch 131/313 | Loss: 4.514163 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00131 | Batch 132/313 | Loss: 4.657331 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00132 | Batch 133/313 | Loss: 5.369167 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00133 | Batch 134/313 | Loss: 4.780405 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00134 | Batch 135/313 | Loss: 4.531930 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00135 | Batch 136/313 | Loss: 4.471125 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00136 | Batch 137/313 | Loss: 4.514806 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00137 | Batch 138/313 | Loss: 4.460052 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00138 | Batch 139/313 | Loss: 4.500774 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00139 | Batch 140/313 | Loss: 4.457355 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00140 | Batch 141/313 | Loss: 4.505357 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00141 | Batch 142/313 | Loss: 4.472248 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00142 | Batch 143/313 | Loss: 4.496278 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00143 | Batch 144/313 | Loss: 4.469815 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00144 | Batch 145/313 | Loss: 4.481612 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00145 | Batch 146/313 | Loss: 4.458130 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00146 | Batch 147/313 | Loss: 4.484960 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00147 | Batch 148/313 | Loss: 4.454133 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00148 | Batch 149/313 | Loss: 4.480113 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00149 | Batch 150/313 | Loss: 4.455340 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00150 | Batch 151/313 | Loss: 4.475430 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00151 | Batch 152/313 | Loss: 4.460472 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00152 | Batch 153/313 | Loss: 4.459853 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00153 | Batch 154/313 | Loss: 4.467585 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00154 | Batch 155/313 | Loss: 4.457358 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00155 | Batch 156/313 | Loss: 4.472903 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00156 | Batch 157/313 | Loss: 4.453068 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00157 | Batch 158/313 | Loss: 4.480842 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00158 | Batch 159/313 | Loss: 4.454934 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00159 | Batch 160/313 | Loss: 4.474272 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00160 | Batch 161/313 | Loss: 4.453732 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00161 | Batch 162/313 | Loss: 4.467849 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00162 | Batch 163/313 | Loss: 4.453765 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00163 | Batch 164/313 | Loss: 4.471027 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00164 | Batch 165/313 | Loss: 4.453455 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00165 | Batch 166/313 | Loss: 4.466646 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00166 | Batch 167/313 | Loss: 4.455164 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00167 | Batch 168/313 | Loss: 4.468966 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00168 | Batch 169/313 | Loss: 4.456603 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00169 | Batch 170/313 | Loss: 4.464917 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00170 | Batch 171/313 | Loss: 4.468117 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00171 | Batch 172/313 | Loss: 4.453609 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00172 | Batch 173/313 | Loss: 4.461665 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00173 | Batch 174/313 | Loss: 4.455116 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00174 | Batch 175/313 | Loss: 4.455647 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00175 | Batch 176/313 | Loss: 4.461410 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00176 | Batch 177/313 | Loss: 4.452265 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00177 | Batch 178/313 | Loss: 4.469923 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00178 | Batch 179/313 | Loss: 4.487546 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00179 | Batch 180/313 | Loss: 4.474976 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00180 | Batch 181/313 | Loss: 4.453305 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00181 | Batch 182/313 | Loss: 4.479743 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00182 | Batch 183/313 | Loss: 4.521257 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00183 | Batch 184/313 | Loss: 4.485801 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00184 | Batch 185/313 | Loss: 4.454714 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00185 | Batch 186/313 | Loss: 4.461954 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00186 | Batch 187/313 | Loss: 4.491694 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00187 | Batch 188/313 | Loss: 4.521141 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00188 | Batch 189/313 | Loss: 4.485641 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00189 | Batch 190/313 | Loss: 4.461745 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00190 | Batch 191/313 | Loss: 4.454318 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00191 | Batch 192/313 | Loss: 4.452672 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00192 | Batch 193/313 | Loss: 4.454183 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00193 | Batch 194/313 | Loss: 4.460690 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00194 | Batch 195/313 | Loss: 4.522319 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00195 | Batch 196/313 | Loss: 4.814891 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00196 | Batch 197/313 | Loss: 4.608184 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00197 | Batch 198/313 | Loss: 7.947641 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00198 | Batch 199/313 | Loss: 4.756426 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00199 | Batch 200/313 | Loss: 4.458648 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00200 | Batch 201/313 | Loss: 4.930688 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00201 | Batch 202/313 | Loss: 4.470135 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00202 | Batch 203/313 | Loss: 4.752825 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00203 | Batch 204/313 | Loss: 4.488739 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00204 | Batch 205/313 | Loss: 4.561193 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00205 | Batch 206/313 | Loss: 4.612073 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00206 | Batch 207/313 | Loss: 4.469468 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00207 | Batch 208/313 | Loss: 4.489991 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00208 | Batch 209/313 | Loss: 4.558922 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00209 | Batch 210/313 | Loss: 4.474023 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00210 | Batch 211/313 | Loss: 4.460938 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00211 | Batch 212/313 | Loss: 4.503772 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00212 | Batch 213/313 | Loss: 4.496887 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00213 | Batch 214/313 | Loss: 4.453289 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00214 | Batch 215/313 | Loss: 4.469138 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00215 | Batch 216/313 | Loss: 4.492681 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00216 | Batch 217/313 | Loss: 4.468646 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00217 | Batch 218/313 | Loss: 4.452210 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00218 | Batch 219/313 | Loss: 4.477577 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00219 | Batch 220/313 | Loss: 4.471266 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00220 | Batch 221/313 | Loss: 4.452446 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00221 | Batch 222/313 | Loss: 4.461685 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00222 | Batch 223/313 | Loss: 4.468286 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00223 | Batch 224/313 | Loss: 4.455237 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00224 | Batch 225/313 | Loss: 4.457253 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00225 | Batch 226/313 | Loss: 4.467336 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00226 | Batch 227/313 | Loss: 4.454436 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00227 | Batch 228/313 | Loss: 4.456092 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00228 | Batch 229/313 | Loss: 4.466636 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00229 | Batch 230/313 | Loss: 4.451858 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00230 | Batch 231/313 | Loss: 4.459777 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00231 | Batch 232/313 | Loss: 4.455547 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00232 | Batch 233/313 | Loss: 4.451479 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00233 | Batch 234/313 | Loss: 4.458422 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00234 | Batch 235/313 | Loss: 4.452780 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00235 | Batch 236/313 | Loss: 4.455147 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00236 | Batch 237/313 | Loss: 4.454431 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00237 | Batch 238/313 | Loss: 4.453017 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00238 | Batch 239/313 | Loss: 4.456720 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00239 | Batch 240/313 | Loss: 4.451680 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00240 | Batch 241/313 | Loss: 4.454496 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00241 | Batch 242/313 | Loss: 4.451846 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00242 | Batch 243/313 | Loss: 4.452393 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00243 | Batch 244/313 | Loss: 4.454590 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00244 | Batch 245/313 | Loss: 4.452430 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00245 | Batch 246/313 | Loss: 4.455650 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00246 | Batch 247/313 | Loss: 4.451913 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00247 | Batch 248/313 | Loss: 4.455725 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00248 | Batch 249/313 | Loss: 4.451398 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00249 | Batch 250/313 | Loss: 4.460040 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00250 | Batch 251/313 | Loss: 4.453073 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00251 | Batch 252/313 | Loss: 4.452011 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00252 | Batch 253/313 | Loss: 4.452615 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00253 | Batch 254/313 | Loss: 4.451057 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00254 | Batch 255/313 | Loss: 4.452473 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00255 | Batch 256/313 | Loss: 4.450657 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00256 | Batch 257/313 | Loss: 4.453164 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00257 | Batch 258/313 | Loss: 4.452587 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00258 | Batch 259/313 | Loss: 4.451867 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00259 | Batch 260/313 | Loss: 4.454775 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00260 | Batch 261/313 | Loss: 4.450744 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00261 | Batch 262/313 | Loss: 4.454381 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00262 | Batch 263/313 | Loss: 4.451218 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00263 | Batch 264/313 | Loss: 4.455447 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00264 | Batch 265/313 | Loss: 4.452498 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00265 | Batch 266/313 | Loss: 4.451617 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00266 | Batch 267/313 | Loss: 4.455249 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00267 | Batch 268/313 | Loss: 4.451008 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00268 | Batch 269/313 | Loss: 4.452039 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00269 | Batch 270/313 | Loss: 4.453259 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00270 | Batch 271/313 | Loss: 4.451636 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00271 | Batch 272/313 | Loss: 4.450770 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00272 | Batch 273/313 | Loss: 4.451159 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00273 | Batch 274/313 | Loss: 4.452721 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00274 | Batch 275/313 | Loss: 4.451758 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00275 | Batch 276/313 | Loss: 4.451767 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00276 | Batch 277/313 | Loss: 4.454010 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00277 | Batch 278/313 | Loss: 4.456301 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00278 | Batch 279/313 | Loss: 4.454687 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00279 | Batch 280/313 | Loss: 4.451688 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00280 | Batch 281/313 | Loss: 4.452291 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00281 | Batch 282/313 | Loss: 4.451375 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00282 | Batch 283/313 | Loss: 4.452787 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00283 | Batch 284/313 | Loss: 4.451930 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00284 | Batch 285/313 | Loss: 4.452255 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00285 | Batch 286/313 | Loss: 4.451264 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00286 | Batch 287/313 | Loss: 4.451164 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00287 | Batch 288/313 | Loss: 4.455241 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00288 | Batch 289/313 | Loss: 4.462828 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00289 | Batch 290/313 | Loss: 4.483696 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00290 | Batch 291/313 | Loss: 4.573320 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00291 | Batch 292/313 | Loss: 4.749833 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00292 | Batch 293/313 | Loss: 5.051488 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00293 | Batch 294/313 | Loss: 4.469572 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00294 | Batch 295/313 | Loss: 4.479618 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00295 | Batch 296/313 | Loss: 5.441536 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00296 | Batch 297/313 | Loss: 8.136517 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00297 | Batch 298/313 | Loss: 7.214062 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00298 | Batch 299/313 | Loss: 7.447586 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00299 | Batch 300/313 | Loss: 5.326379 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00300 | Batch 301/313 | Loss: 4.490058 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00301 | Batch 302/313 | Loss: 4.652411 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00302 | Batch 303/313 | Loss: 4.451328 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00303 | Batch 304/313 | Loss: 4.645199 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00304 | Batch 305/313 | Loss: 4.802871 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00305 | Batch 306/313 | Loss: 4.677340 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00306 | Batch 307/313 | Loss: 4.491498 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00307 | Batch 308/313 | Loss: 4.462744 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00308 | Batch 309/313 | Loss: 4.568997 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00309 | Batch 310/313 | Loss: 4.624990 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00310 | Batch 311/313 | Loss: 4.548676 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00311 | Batch 312/313 | Loss: 4.458935 | LR: 0.00010000 | BS: 32
🚀 Epoch 01/30 | Step 00312 | Batch 313/313 | Loss: 4.039244 | LR: 0.00010000 | BS: 16

--- End of epoch validation ---
Validation batch 0: Loss=4.473765
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/train.py", line 434, in <module>
    results = engine.train(model=siglip,
  File "/speech/advait/yuvraj/LLMs/SmolSigLip/engine.py", line 323, in train
    'Ratio': f'{test_loss/train_loss:.3f}' if train_loss > 0 else 'N/A'
RuntimeError: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.
